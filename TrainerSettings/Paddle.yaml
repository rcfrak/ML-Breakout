# A basic PPO trainer is configured here and a default section is supplied.
#
# For detailed description of parameters, see below link
# https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#common-trainer-configurations
#
# Experiment using the behaviors section and fall back on default as needed.
#
# Usage from Conda Powershell terminal follows:
# mlagents-learn <trainer-config-file> --run-id=<run-identifier>
# where <trainer-config-file> is the local path to this file.
#
# Inline comments denote common ranges for each parameter

default_settings:
  Paddle:
    trainer_type: ppo
    
    hyperparameters:
      batch_size: 32 # 32 - 512
      buffer_size: 2048 # 2048 - 409600
      learning_rate: 0.0003 # 1E-5 - 1E-3
      learning_rate_schedule: linear 
      beta: 0.005 # 1E-4 - 1E-2
      epsilon: 0.2 # 0.1 - 0.3
      lambd: 0.95 # 0.9 - 0.95
      num_epoch: 3 # 3 - 10
    
    network_settings:
      normalize: false
      hidden_units: 32 # 32 - 512
      num_layers: 1 # 1 - 3
      vis_encode_type: simple
    
    reward_signals:
      extrinsic:
        gamma: 0.99 # 0.8 - 0.995
        strength: 1.0
    
    keep_checkpoints: 5
    max_steps: 500000 # 5E5 - 1E7
    time_horizon: 32 # 32 -2048
    summary_freq: 2000 #50000

behaviors:
  # Behaviors added here will overwrite those in the default section
